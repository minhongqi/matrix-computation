\documentclass[english,onecolumn]{IEEEtran}
\usepackage[T1]{fontenc}
\usepackage[latin9]{luainputenc}
\usepackage[letterpaper]{geometry}
\geometry{verbose}
\usepackage{amsfonts}
\usepackage{babel}
\usepackage{ulem}

\usepackage{extarrows}
\usepackage[colorlinks]{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[ruled,linesnumbered]{algorithm2e}

\usepackage{amsmath,graphicx}
\usepackage{subfigure} 
\usepackage{cite}
\usepackage{amsthm,amssymb,amsfonts}
\usepackage{textcomp}
\usepackage{bm,pifont}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xparse}
\usepackage{cleveref}
\usepackage{xcolor}
\usepackage[center]{caption}

\definecolor{salmon}{rgb}{1, 0.5020, 0.4471}

\NewDocumentCommand{\codeword}{v}{
\texttt{\textcolor{blue}{#1}}
}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
\topmargin            -18.0mm
\textheight           226.0mm
\oddsidemargin      -4.0mm
\textwidth            166.0mm
\def\baselinestretch{1.5}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\Pb}{\mathbf{P}}
\newcommand{\Ib}{\mathbf{I}}
\newcommand{\vb}{\mathbf{v}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Wcal}{\mathcal{W}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\bigS}{\mathcal{S}}
\newcommand{\bA}{{\bf A}}
% \newcommand{\bU}{{\bf U}}
\newcommand{\bV}{{\bf V}}
\newcommand{\bY}{{\bf Y}}
\newcommand{\bX}{{\bf X}}
\newcommand{\bmu}{{\boldsymbol \mu}}
\newcommand{\bSig}{{\boldsymbol \Sigma}}
\newcommand{\bone}{{\boldsymbol 1}}
\newcommand{\bQ}{{\bf Q}}
\newcommand{\bR}{{\bf R}}
\newcommand{\bH}{{\bf H}}
\newcommand{\bU}{{\bf U}}
\newcommand{\bT}{{\bf T}}
\newcommand{\bI}{{\bf I}}
\newcommand{\bq}{{\bf q}}
\newcommand{\bz}{{\bf z}}
\newcommand{\bL}{{\bf L}}
\newcommand{\bx}{{\bf x}}
\newcommand{\bv}{{\bf v}}
\newcommand{\bG}{{\bf G}}
\def\A{\mathbf{A}}
\def\v{\mathbf{v}}

\begin{document}

\begin{center}
	\textbf{\LARGE{SI231 - Matrix Computations, Fall 2020-21}}\\
	{\Large Homework Set \#5}\\
	\texttt{Prof. Yue Qiu and Prof. Ziping Zhao}\\
	\texttt{\textbf{Name:}}   	\texttt{ Min Hongqi }  		\hspace{1bp}
	\texttt{\textbf{Major:}}  	\texttt{ EE } 	\\
	\texttt{\textbf{Student No.:}} 	\texttt{ 2020E8018482005}     \hspace{1bp}
	\texttt{\textbf{E-mail:}} 	\texttt{ minhq@sari.ac.cn}
\par\end{center}

\noindent
\rule{\linewidth}{0.4pt}
{\bf {\large Acknowledgements:}}
\begin{enumerate}
    \item Deadline: \textcolor{red}{\textbf{2020-12-07 23:59:00}}
    \item Submit your homework at \textbf{Gradescope}.
    Homework \#5 contains two parts, the theoretical part the and the programming part.
    \item About the the theoretical part:
    \begin{enumerate}
            \item[(a)] Submit your homework in \textbf{Homework 5} in gradescope. Make sure that you have correctly select pages for each problem. If not, you probably will get 0 point.
            \item[(b)] Your homework should be uploaded in the \textbf{PDF} format, and the naming format of the file is not specified.
            \item[(c)] No handwritten homework is accepted. You need to use \LaTeX $\,$ in principle.
            \item[(d)] Use the given template and give your solution in English. Solution in Chinese is not allowed. 
        \end{enumerate}
  \item About the programming part:
  \begin{enumerate}
      \item[(a)] Submit your codes in \textbf{Homework 5 Programming part} in gradescope. 
      \item[(b)] When handing in your homework in gradescope, package all your codes into {\sf your\_student\_id+hw4\_code.zip} and upload. In the package, you also need to include a file named {\sf README.txt/md} to clearly identify the function of each file. (\textcolor{blue}{".zip" format package rather than ".rar, .7zip" should be uploaded, solution of the results should be named according to requirements}.)
     \item[(c)] Make sure that your codes can run and are consistent with your solutions.
  \end{enumerate}
  \item \textbf{Late Policy details can be found in the bulletin board of Blackboard.}
\end{enumerate}
\rule{\linewidth}{0.4pt}

\section*{Study Guide}
\noindent
This homework concerns the following topics:
\begin{itemize}
    \item \textbf{Regularization}, see Lecture 8 of Zhao, Lecture 20 of Qiu.
    \item \textbf{SVD computation}, see Lecture 7 of Zhao, Lecture 18 of Qiu. 
    \item \textbf{Iterative methods}, see Lecture 2 of Zhao, Lecture 21 of Qiu. 
    \item \textbf{Application of SVD, PCA}, see Lecture 7 of Zhao, Lecture 19 of Qiu. 
\end{itemize}

\newpage
\section{Least square with regularization}
\noindent\textbf{Problem 1}. 
In this problem, we will learn how to solve LS when there is noise. for some $\lambda>0$, where the term $\lambda||\mathbf{x}||^{k}_{k}$ is added to improve the system conditioning, thereby attempting to reduce noise sensitivity. Usually, we set $k=1$ which is called $\ell$1 regularization, or set $k=2$ which is called $\ell$2 regularization. 

\noindent $\ell$1 regularized LS:
\begin{equation}
    \mathbf{x_{LS}}=\min_{\mathbf{X}\in\mathbf{\Rbb}^{n}}f(\mathbf{x}), \quad f(\mathbf{x})=||\mathbf{Ax-y}||^{2}_{2}+\lambda||\mathbf{x}||_{1}
\end{equation}
\noindent $\ell$2 regularized LS:
\begin{equation}
    \mathbf{x_{LS}}=\min_{\mathbf{X}\in\mathbf{\Rbb}^{n}}f(\mathbf{x}),\quad f(\mathbf{x})=||\mathbf{Ax-y}||^{2}_{2}+\lambda||\mathbf{x}||^{2}_{2}
\end{equation}

Write programs to solve the regularized least square problem, any programming language is suitable. where $\mathbf{A} \in \Rbb^{m \times n}$ is a matrix representing the predefined data set with $m$ data samples of $n$ dimensions ($m$=1000, $n$=210), and $\mathbf{y} \in \Rbb^m$ represents the labels. The data samples are provided in the "data.txt" file, and the labels are provided in the "label.txt" file, you are supposed to load the data before solving the problem. In this problem, we set $\lambda=0.1$ for two algorithms.
\begin{enumerate}
    \item (\textcolor{blue}{\textbf{8 points}}) Solve $\ell$1 regularized LS using Majorization-Minimization method, we set $c=1e+5$ in this problem. The Majorization-Minimization method for solving problem updates $ {\bf x}$ as
    $$
        {\bf x^{(k+1)}} = \mathbf{soft}(\frac{1}{c}\mathbf{A^{T}}(\mathbf{y}-\mathbf{Ax^{(k)}})+\mathbf{x^{(k)}},\lambda/c),
    $$
 where $\mathbf{soft}$ is called the soft-thresholding operator and is defined as follows: if $\mathbf{z=soft(x,\sigma)}$, then $z_{i}=sign(x_{i})\max\{|x_{i}|-\sigma,0\}$.
    \item (\textcolor{blue}{\textbf{8 points}}) Solve $\ell$2 regularized LS using gradient descent method. The gradient descent method for solving problem updates $ {\bf x}$ as
    $$
        {\bf x} = {\bf x} - \gamma \cdot \nabla_{{\bf x}} f(\mathbf{x}),
    $$
 where $\gamma$ is the step size of the gradient decent methods. We suggest that you can set $\gamma=1e-5$. 
    \item (\textcolor{blue}{\textbf{4 points}}) Compare two methods above. 
    \begin{enumerate}
        \item[(a)] compare L0 norm (the number of non-zero elements) of $\mathbf{x_{LS}}$ computed by above two algorithms;
        \item[(b)] Compare the loss $||\mathbf{Ax-y}||^{2}_{2}$ for results $\mathbf{x}=\mathbf{x_{LS}}$ of above two algorithms.
    \end{enumerate}
\end{enumerate}

\noindent\textbf{Remarks: }
\begin{itemize}
    \item The solution of the two methods should be printed in files named "sol1.txt" and "sol2.txt" and submitted in gradescope.  The format should be same as the input file (210 rows plain text, each rows is a dimension of the final solution).
    \item Make sure that your codes are executable and are consistent with your solutions.
\end{itemize}

\noindent
\textbf{Solution.}
\begin{enumerate}
	\item
	\item
	\item
	\begin{enumerate}
		\item[(a)] Both L0 norm of $\mathbf{x_{LS}}$ two algorithms are 0.
		\item[(b)] In the Majorization-Minimization method, we get the loss $||\mathbf{Ax-y}||^{2}_{2} = 2.9453e+03$,\\
		In the gradient decent method,  we get the loss $||\mathbf{Ax-y}||^{2}_{2} = 2.9455e+03$.
	\end{enumerate}	
\end{enumerate}

\newpage
\section{Computations of SVD}
\noindent\textbf{Problem 2}.
In this problem you will see singular value stability and discover computation of SVD.
\begin{enumerate}
    \item (\textcolor{blue}{\textbf{4 points}}) Consider a $4 \times 4$ matrix
    \[
	\mathbf{A} = \begin{bmatrix}
		0& 1&0&0 \\
		0&0&2&0\\
		0&0&0&3\\
		\frac{1}{6000}&0&0&0
	\end{bmatrix}\,,
	\]
	find its singular values and eigenvalues. In this sub-problem you can use eigenvalue decomposition ('eig') and singular value decomposition function ('svd') in Matlab or Python.
	\item (\textcolor{blue}{\textbf{8 points}}) Compute the SVD decomposition of a $3 \times 2$ matrix by \Cref{alg:svd}
	\[
	\mathbf{B} = \begin{bmatrix}
		3& 2 \\
		1&4  \\
		\frac{1}{2}&\frac{1}{2}
	\end{bmatrix}\,.
	\]
Are the results right? If not, can you get the correct answer through the results of \Cref{alg:svd}? The In this sub-problem you can use the eigenvalue decomposition ('eig') and QR decomposition function ('qr') in Matlab or Python.
\item (\textcolor{blue}{\textbf{8 points}}) Consider an $m\times m$ upper triangular matrix with 0.1 on the main diagonal and 1 everywhere above the diagonal. Fine the smallest singular value through the singular value decomposition function ('svd') and \cref{alg:svd}. You are required to plot two curves on a log scale for $m=1,\dots,30$ and show which one is the desired result.
\end{enumerate}

\begin{algorithm}[htbp]
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\caption{SVD Decomposition by $\mathbf{A}^T\mathbf{A}$}
\label{alg:svd}
\SetAlgoLined
\Input{Thin matrix A}
Form $\mathbf{A}^T\mathbf{A}$.\\
Compute the eigenvalue decomposition $\mathbf{A}^T\mathbf{A}$=$\mathbf{V}^T\mathbf{\Lambda V}$.\\
Let $\mathbf{\Sigma}$ be an m$\times$n diagonal matrix with diagonal entries being the nonnegative square root of diagonal entries of $\mathbf{\Lambda}$.\\
Solve the system $\mathbf{U\Sigma} = \mathbf{AV} $for orthogonal matrix $\mathbf{U}$ (e.g., via QR factorization). \\
\Output{$\mathbf{U}, \mathbf{\Sigma}, \mathbf{V}$.}

\end{algorithm}
{\bf Remarks}
\begin{enumerate}
    \item Singular values cannot be negative.
    \item Please show your result and insert figures in your PDF.
\end{enumerate}

\noindent
\textbf{Solution.}

\begin{enumerate}
	\item
	Input $[V, D]=\operatorname{eig}(A)$ in matlab,\\
	we get $D=\left[\begin{array}{cccc}-0.1778 & & & \\ & 0.1778 i & & \\ & & -0.1778 i & \\ & & & 0.1778\end{array}\right]$\\\\
	so the eigenvalues are: -0.1778, 0.1778i, -0.1778i, 0.1778\\
	Input $[U,S,V]=\operatorname{svd}(A)$ in matlab,\\
	we get $S=\left[\begin{array}{llll}3 & & \\ & 2 & \\ & & 1 \\ & & & 0.0002\end{array}\right]$\\\\
	so the singular values are: 3, 2, 1, 0.0002\\
	
	\item
	$B^T B=\left[\begin{array}{ll}10.25 & 10.25 \\ 10.25 & 20.25\end{array}\right]$\\\\
	Input $[V, D]=\operatorname{eig}\left(B^T B\right)$ in matlab,\\\\
	we get $V=\left[\begin{array}{ll}-0.8481 & 0.5299 \\ 0.5299 & 0.8481\end{array}\right] \quad D=\left[\begin{array}{ll}3.8455 &\\ &26.6545\end{array}\right]$\\\\
	In step 3, we get $\Sigma=\left[ \begin{array}{rr}1.9610 & 0 \\ 0 & 5.1628 \\ 0 & 0\end{array}\right]$\\\\
	In put $[U, \Sigma]=q r(B V)$ in matlab,\\\\
	we get $U=\left[\begin{array}{rrr} -0.7570 & -0.6364 & -0.1482 \\ 0.6484 & -0.7597 & -0.0494 \\ -0.0811 & -0.1335 & 0.9877\end{array}\right]$\\\\
	In order to verify the correctness of SVD decomposition, calculate $\mathbf{U\Sigma V^T}=\left[\begin{array}{rr}-0.4823 & -3.5732 \\ -3.1567 & -2.6525 \\ -0.2302 & -0.6686\end{array}\right]$	\\\\
	Obviously, the results are wrong.\\
	$\bold{Correction:}$\\
	If we sort the eigenvalues of D in order from large to small in step 2, then the column vectors of V are going to changing the order, we will get:
	$V=\left[\begin{array}{ll}0.5299 & -0.8481 \\ 0.8481 &0.5299 \end{array}\right] \quad D=\left[\begin{array}{ll} 26.6545&\\ &3.8455\end{array}\right]$\\\\
	then sort the square root of diagonal entries of $\mathbf{\Lambda}$ in order from large to small, get $\Sigma=\left[ \begin{array}{rr} 5.1628 & 0 \\ 0 & 1.9610\\ 0 & 0\end{array}\right]$\\\\
	Continue with steps 4, we get $U=\left[\begin{array}{rrr} -0.6364 & 0.7570 & -0.1482 \\
		-0.7597 & -0.6484 & -0.0494 \\
		-0.1335 & 0.0811 & 0.9877\end{array}\right]$\\\\
	and $\mathbf{U\Sigma V^T}=\left[\begin{array}{rr}-3.0000 & -2.0000 \\ -1.0000 & -4.0000 \\ -0.5000 & -0.5000\end{array}\right]$	\\\\
	Although there is a minus sign difference between $\mathbf{U\Sigma V^T}$ and A, but the results can be considered right.
	
	\item Plot two curves on a log scale for $m=1,\dots,30$, \\
	Because when $m=10,12,13,20,22,23,25,27$,  the eigenvaluses calculated by Matlab of $\mathbf{A}^T\mathbf{A}$ will be negative, so it's nonsense to calculate the nonnegative square of diagonal entries of $\mathbf{\Lambda}$.\\ 
	So the smallest singular value through SVD is the desired result.
	\begin{figure}[hbp]
		\centering
		\includegraphics[height=8cm,width=12cm]{pro2_3.png}
		\caption{The smallest singular value through Algorithm 1 and SVD}
		\label{1}
	\end{figure} 

	
	
	

\end{enumerate}



\clearpage
\newpage
\section{Iterative methods.}
\noindent\textbf{Problem 3}.
In this problem, you will learn how to implement there iterative methods, Jacobi, Gauss-Seidel and SOR Methods to solve the linear equation $\mathbf{b}=\mathbf{Ax}$, and compare the convergence of these methods. 
"A1.txt", "A2.txt","b1.txt", "b2.txt","x1.txt", "x2.txt" is the data of the $\mathbf{b}, \mathbf{x}, \mathbf{A}$ with different size (10 and 1000 respectively).
\begin{algorithm}
	\caption{Iterations method}
	\label{algo:Jacobi}
    \LinesNumbered
	\KwIn{A starting point $\mathbf{x}^{0}$, maximum iterations $N$, error bound $\mathrm{eps}$}
    \KwOut{Solution $\mathbf{x}$ of $\mathbf{Ax}=\mathbf{b}$}
	\For{$k=0,1,2,\dots,N$}{
	    \textbf{update} $\mathbf{x}^k$ to obtain $\mathbf{x}^{k+1}$ \tcp*{Use one of the three iteration methods}
        \If{$\|\mathbf{x}^{k+1}-\mathbf{x}^{k}\|_2< \mathrm{eps}$ }{
            \textbf{break}\;
        }
    }
\end{algorithm}

Suppose $\mathbf{x}\in\mathbb{R}^{n}$, $\mathbf{A}=[a_{ij}]\in\mathbb{R}^{n\times n}$ and $\mathbf{b}=(b_i)\in\mathbb{R}^{n}$.

Each iteration method uses different updates:

For Jacobi Iteration, 
\begin{equation*}
    x_i^{k+1} = (b_i - \sum_{i \neq j} a_{ij}x_j^k) / a_{ii} \quad \text{for }\ i = 1,\dots, n,
\end{equation*}

For Gauss-Seidel Iteration, 
\begin{equation*}
    x_i^{k+1} = (b_i - \sum_{j > i} a_{ij}x_j^{k+1} - \sum_{j < i} a_{ij}x_j^{k}) / a_{ii}, \quad \text{for }\ i = 1,\dots, n,
\end{equation*}

For SOR Iteration with relaxation factor $\omega$, 
\begin{equation*}
    x_i^{k+1} = (1-\omega) x_i^{k} + \omega \left(b_i - \sum_{j>i} a_{ij}x_j^{k+1} - \sum_{j<i} a_{ij}x_j^{k}\right) / a_{ii}, \quad \text{for }\ i = 1,\dots, n.
\end{equation*}

\textbf{Remarks:}
\begin{enumerate}
    \item (\textcolor{blue}{\textbf{8 points}}) Implement the Jacobi Iteration and Gauss-Seidel Iteration methods.
    \item (\textcolor{blue}{\textbf{8 points}}) Implement the SOR Iteration method, tune the relaxation factor $\omega$ to achieve high convergence rate and plot the curve of number of iterations v.s. the value of $\omega$.
    \item (\textcolor{blue}{\textbf{4 points}}) Plot the error $\|\mathbf{x}_{\mathrm{true}} - \mathbf{x}^k\|_2$ for $k=1,2,\dots,N$, analyze the convergence rate and the final error of three methods.
\end{enumerate}
Note: The value of the ``eps'' in Terminal condition should not be too large!



\noindent
\textbf{Solution.}
\begin{enumerate}
	\item
	
	\item From the \Cref{2}, set $eps = 1e-5$, we can see that when $w=1$, we can achieve the highest convergence rate.
	\begin{figure}[h]
		\centering
		\includegraphics[height=7cm,width=11cm]{pro3_2.png}
		\caption{The smallest singular value through Algorithm 1 and SVD}
		\label{2}
	\end{figure}


	\item From the \Cref{3}, set $eps = 1e-20$ and $w=0.9$, then plot the curves on a log scale, we find convergence of Gauss-Seidel Iteration is faster than Jacobi.\\
	When $n=10$, the convergence of SOR is between Gauss-Seidel Iteration and Jacobi method,\\
	When $n=1000$, the convergence of SOR id the slowest.
	\begin{figure}[h]
		\centering
		\includegraphics[height=7cm,width=11cm]{pro3_3_1.png}
		\caption{Analyze the convergence rate for n=10}
		\label{3}
	\end{figure}
\clearpage
\begin{figure}[h]
	\centering
	\includegraphics[height=7cm,width=10cm]{pro3_3_2.png}
	\caption{Analyze the convergence rate for n=1000}
	\label{4}
\end{figure}
Give the final error $\|\mathbf{x}_{\mathrm{true}} - \mathbf{x}^k\|_2$ for $k=N$.\\
N=10 :
\begin{enumerate}
	\item Jacobi: $\|\mathbf{x}_{\mathrm{true}} - \mathbf{x}^k\|_2=1.6725e-16$
	\item Gauss-Seidel: $\|\mathbf{x}_{\mathrm{true}} - \mathbf{x}^k\|_2=1.6725e-16$
	\item SOR: $\|\mathbf{x}_{\mathrm{true}} - \mathbf{x}^k\|_2=2.0266e-16$
\end{enumerate}
N=1000 :
\begin{enumerate}
	\item Jacobi: $\|\mathbf{x}_{\mathrm{true}} - \mathbf{x}^k\|_2=7.7727e-15$
	\item Gauss-Seidel: $\|\mathbf{x}_{\mathrm{true}} - \mathbf{x}^k\|_2=7.8601e-15$
	\item SOR: $\|\mathbf{x}_{\mathrm{true}} - \mathbf{x}^k\|_2=7.9653e-15$
\end{enumerate}
\end{enumerate}


\clearpage
\section{Application of SVD}
\noindent\textbf{Problem 4}.
In this problem you will see one of the application of SVD, what is doing face recognition on Yale B dataset. Here we will only use $4$ individuals under $15$ relatively brighter different illumination conditions.

\begin{figure}[h]
    \centering
    \includegraphics{figures/example_face.png}
    \caption{Face images of one individual under 5 different illumination conditions in the extended YaleB dataset. All images are frontal faces.}
    \label{fig:my_label}
\end{figure}
\vspace{-0.3cm}
Face recongition is an area of computer vision in which low-dimensional linear models such as principal component analysis (PCA) and its variations have been popular tools for capturing the variability of face images. And it has been shown that PCA can be solved by SVD optimally. In the following questions you will see how \Cref{Alg:PCA} can be used.

\vspace{0.5cm}
\begin{algorithm}[H]
\textbf{Parameters}:\\
$\bX$: \quad $D\times N $ data matrix.\\
$d$: \quad Number of principal components.\\
\textbf{Returned values}:\\
$\bmu$: \quad Mean of the data.\\
$\bU$: \quad Orthonormal basis for the subspace.\\
$\bY$: \quad Low-dimensional representation( or principal components).\\
\textbf{Description}:\\
Compute the SVD of the data matrix $\bX - \bmu \bone^T = \bU \bSig \bV^T$.\\
Sort the singular values in descending order, choose the first $d$ singular values and the corresponding $d$ left singular vectors $\bU_d$.\\
Find the $d$ principal components $\bY = \bU_d^T(\bX - \bmu \bone^T)$
\caption{[$\bmu, \bU, \bY$] = PCA\_via\_SVD($\bX,d$) }
\label{Alg:PCA}
\end{algorithm}
\vspace{0.5cm}

The principal bases $\bU$ estimated by PCA are also known as the \textit{eigenfaces} in the computer vision literature. The first eigenface is the left singular vector corresponding to the largest singular value, the second eigenface is the left singular vector corresponding to the second largest singular value and so on.

\begin{enumerate}
    \item (\textcolor{blue}{\textbf{4 points}}) Apply PCA in \Cref{Alg:PCA} with $d = 10$ to individual 4\footnote{you can use the images or the mat file in data1 folder}. Plot the mean face $\bmu$, the first eigenface, the second eigenface, the third eigenface and the tenth eigenface. Describe what you have observed.
    \item (\textcolor{blue}{\textbf{8 points}}) Apply PCA in \Cref{Alg:PCA} with $d = 10$ to the Training Set. 
    \begin{enumerate}
        \item Plot the mean face, the first eigenface, the second eigenface, the third eigenface and the tenth eigenface.
        \item Plot the sorted singular values.
        \item Project the Test Set onto the face subspace given by PCA, that is $\bY_{test} = \bU^T(\bX_{test}-\bmu \bone^T)$. Compute the projected faces, that is $\text{Proj}(\bX_{test}) = \bmu \bone^T + \bU\bY_{test}$. Then choose one projected face for each individual and plot them. And show those projected faces for $d = 2$ again.
    \end{enumerate} 
    \item (\textcolor{blue}{\textbf{8 points}}) Classify these test faces using 1-nearest-neighbor, that is, label an image $x$ as corresponding to individual $i$ if its projected image $y$ is closest to one of projected training image $y_j$ of individual $i$\footnote{we use $\ell_2$ norm as the distance measurement: $ \|y-y_j\|_2$}. Report the percentage of incorrectly classified face images for $d = 2,...,8$ and put them into the following table. Which value of $d$ gives the best recognition performance?
    \begin{table}[h]
        \centering
        \caption{Summary of errors}
        \begin{tabular}{c|c|c|c|c|c|c|c}
        \# eigenvectors& 2 & 3& 4 & 5 & 6 & 7 & 8 \\ 
        \hline
            error &  &  &  &  &  &  &  
        \end{tabular}
        \label{tab:my_label}
    \end{table}
\end{enumerate}
{\bf Remarks:}
\begin{enumerate}
    \item Data are provided in two forms: images and mat data. You only need to use one of them. 
    \item $10$ images for each individuals in the training data, and $5$ images for each individuals in the test data. All images are of the same size $48\times 42$.
    \item In mat file \textbf{all\_data.mat}, \textbf{data\_train} and \textbf{data\_test} are the matrices of training data and test data where each column is the column stack of an image\footnote{the column stack of an image matrix $\bX\in \Rbb^{m\times n}$ is denoted by $x = [X_{11},X_{21},X_{31},...,X_{mn}]^T\in\Rbb^{mn}$}, \textbf{Y\_label\_train} and \textbf{Y\_label\_test} are ground truth labels.
    \item Recommend to use truncated(thin) SVD for fast implementation.
    \item Please \textbf{insert your figures in your PDF}.
\end{enumerate}

\noindent
\textbf{Solution.}
\begin{enumerate}	
	\newpage
	\item The first eigenface resembles the mean face best. The bigger the singular value is, the eigenface is more similar to the meanface.
	\begin{figure}[h]
		\centering
		\includegraphics[height=4cm,width=3.5cm]{pro4_1_1.png}
		\caption{The mean face}
		\label{5}
	\end{figure}

	\begin{figure}[htbp]
		\centering
		\subfigure[The first eigen face]{
			\includegraphics[width=3.5cm]{pro4_1_2.png}
			%\caption{fig1}
		}
		\quad
		\subfigure[The second eigen face]{
			\includegraphics[width=3.5cm]{pro4_1_3.png}
		}
		\quad
		\subfigure[The third eigen face]{
			\includegraphics[width=3.5cm]{pro4_1_4.png}
		}
		\quad
		\subfigure[The tenth eigen face]{
			\includegraphics[width=3.5cm]{pro4_1_5.png}
		}
		\caption{ Eigenfaces}
	\end{figure}

	\item
	\begin{enumerate}
		\item The mean face and the eigenfaces of the training set.
	\begin{figure}[h]
		\centering
		\includegraphics[height=4cm,width=3.5cm]{pro4_2_1.png}
		\caption{The training set mean face}
		\label{5}
	\end{figure}
	\newpage
	\begin{figure}[htbp]
		\centering
		\subfigure[The first eigen face]{
			\includegraphics[width=3.5cm]{pro4_2_3.png}
			%\caption{fig1}
		}
		\quad
		\subfigure[The second eigen face]{
			\includegraphics[width=3.5cm]{pro4_2_4.png}
		}
		\quad
		\subfigure[The third eigen face]{
			\includegraphics[width=3.5cm]{pro4_2_5.png}
		}
		\quad
		\subfigure[The forth eigen face]{
			\includegraphics[width=3.5cm]{pro4_2_6.png}
		}
		\caption{ Eigenfaces}
	\end{figure}
	\item The the sorted singular values is showed in Figure 10.
	\begin{figure}[h]
		\centering
		\includegraphics[height=6cm,width=7cm]{pro4_2_2.png}
		\caption{The training set mean face}
		\label{5}
	\end{figure}
\newpage
	\item The projected faces for d=10 are as follows. It is clear that they are faces from four different individuals.\\
	\begin{figure}[h]
		\centering
		\subfigure[The first eigen face]{
			\includegraphics[width=3.5cm]{pro4_2_7.png}
			%\caption{fig1}
		}
		\quad
		\subfigure[The second eigen face]{
			\includegraphics[width=3.5cm]{pro4_2_8.png}
		}
		\quad
		\subfigure[The third eigen face]{
			\includegraphics[width=3.5cm]{pro4_2_9.png}
		}
		\quad
		\subfigure[The forth eigen face]{
			\includegraphics[width=3.5cm]{pro4_2_10.png}
		}
		\caption{ Eigenfaces}
	\end{figure}

	d=2 projected faces are more ambiguous. Because the biggest 10 singular values can perform most of the features of the faces, while the biggest 2 can't.
	\begin{figure}[h]
	\centering
	\subfigure[The first eigen face]{
		\includegraphics[width=3.5cm]{pro4_2_11.png}
		%\caption{fig1}
	}
	\quad
	\subfigure[The second eigen face]{
		\includegraphics[width=3.5cm]{pro4_2_12.png}
	}
	\quad
	\subfigure[The third eigen face]{
		\includegraphics[width=3.5cm]{pro4_2_13.png}
	}
	\quad
	\subfigure[The forth eigen face]{
		\includegraphics[width=3.5cm]{pro4_2_14.png}
	}
	\caption{ Eigenfaces}
\end{figure}

	\end{enumerate}
	\item $d=7$ gives the best recognition performance
	\begin{table}[h]
		\centering
		\caption{Summary of errors}
		\begin{tabular}{c|c|c|c|c|c|c|c}
			\# eigenvectors& 2 & 3& 4 & 5 & 6 & 7 & 8 \\ 
			\hline
			error & 55\% & 45\% & 40\% & 25\% & 15\% & 0 & 0  
		\end{tabular}
		\label{tab:my_label}
	\end{table}
	
	
	
	
\end{enumerate}
\clearpage
\newpage
\noindent\textbf{Problem 5}.
In this problem, we will learn how to use SVD to compress, or reduce the dimension of the data. The real data is with low rank internal, but often corrupted, or contaminated by noise, which leads to the full rank of the data matrix $\mathbf{A}\in\mathbb{R}^{D\times N}$ . To reduce the storage consumption of the real data (note that $D$ is extremely large in practical), we seek to find a low rank approximation to  $\mathbf{A}$, that is, we want to solve the problem
\begin{align}
    \min_{\mathbf{X}\in\mathbb{R}^{D\times N}}&\quad \|\mathbf{X}-\mathbf{A}\|_F^2\\
    \mathrm{s.t.}&\quad \mathrm{rank}(\mathbf{X})\leq d
\end{align}
where $\|\cdot\|_F$ is the Frobenius norm, $d<< D$ is unknown.
By the Theorem of PCA via SVD, the above problem is equivalent to the following problem (suppose the eigenvalues of $\mathbf{A}$ are given by $\sigma_1(\mathbf{A})\geq\sigma_2(\mathbf{A})\geq\cdots\geq \sigma_K(\mathbf{A})\geq0$ and $K=\min\{D,N\}$):
\begin{equation}
    \min_{d=1,\dots,K}J(d)=\sum_{i=d+1}^K\sigma_i^2(\mathbf{A})
\end{equation}
However, this is not a good criterion, since the optimal solution is given by $d^*=\mathrm{rank}(\mathbf{A})$ ($J(d^*)=0$).

The problem of determining the optimal dimension $d$ is in fact a "model selection" problem, which is due to the fact that choice of $d$ balances the complexity of the model and the storage of the data. There are many model selection criterion, we will learn two of them.

\begin{enumerate}
    \item The first way is to bound the residual of approximation, that is, suppose $\hat{\mathbf{X}}$ is the best $\mathrm{rank}-d$ approximation of $\mathbf{A}$, given a threshold $\tau>0$, we want to minimize the residual with respect to Frobinius-norm, that is
    \begin{align}
        \min_{d=1,\dots,K}&\quad \|\mathbf{A}-\hat{\mathbf{X}}\|_F^2\leq \tau
    \end{align}
    the problem can be explicitly written as
    \begin{equation}\label{eq:2_norm_min}
        d^*=\min_{d=1,\dots,K}\left\{d\left|\sum_{i=d+1}^K\sigma_{i}^2(\mathbf{A})\leq \tau\right.\right\}
    \end{equation}
    
    \item Note that the first criterion (\ref{eq:2_norm_min}) depends on specific problem (singular values of data matrix are not invariant with respect to linear transformations), it is hard to chose a reasonable threshold. To generalize our criterion, we consider the normalized version of (\ref{eq:2_norm_min}), this new criterion, \textit{a.k.a} \textbf{variance explained ratio} in machine learning, is given by
    \begin{equation}\label{eq:accumulated_error}
        d^*=\min_{d=1,\dots,K}\left\{d\left|\frac{\sum_{i=d+1}^K\sigma_{i}^2(\mathbf{A})}{\sum_{i=1}^K\sigma_i^2(\mathbf{A})}\leq \tau\right.\right\}
    \end{equation}
\end{enumerate}
Now, suppose the data matrix $\mathbf{A}\in\mathbb{R}^{D\times N}$ is the same as {\color{blue} Problem 3} (\emph{data1/data1.mat}). You are required to 
\begin{enumerate}
    \item (\textcolor{blue}{\textbf{8 points}}) Plot the squared singular values of $\mathbf{A}$  along with the threshold $\tau=150$, what is the solution to (\ref{eq:2_norm_min})?
    \item (\textcolor{blue}{\textbf{12 points}, \textbf{6 points} for plot, \textbf{6 points} for table}) Plot the figure of function 
    \begin{equation}
        f(d)=\frac{\sum_{i=d+1}^K\sigma_{i}^2(\mathbf{A})}{\sum_{i=1}^K\sigma_i^2(\mathbf{A})},\quad d=1,\dots,K
    \end{equation}
    and fill the following table from the figure with respect to (\ref{eq:accumulated_error}):
    \begin{table}[htb]
    \centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
    \multicolumn{1}{|c|}{$\tau$} & 0.1 & 0.05 & 0.02 & 0.005 \\ \hline
    Compression rate             &   &    &   &  \\ \hline
    \end{tabular}
    \caption{The compression rate with respect different threshold}
    \end{table}
    
    where the compression rate is defined as 
    \begin{equation}
        \text{compression rate}=\frac{\#\{\text{entries in } \hat{\mathbf{X}}\}}{\#\{\text{entries in } \mathbf{A}\}}=\frac{d^*(D+N+1)}{DN}
    \end{equation}
\end{enumerate}



{\bf Remarks}
\begin{enumerate}
    \item Please \textbf{insert your figures in your PDF}.
    \item You can just mark the solution of problem 1) on your figure (\textbf{plot a vertical line with red color or mark the solution with marker}).
    \item Please draw a continuous curve for problem 2), see \textit{stairs} function in Matlab, \textit{plt.step} function in Python for more details.
    \item For simplicity, we omit some proof details, you may refer to \textit{Generalized Principal Component Analysis, Section 2.1} for more details.
\end{enumerate}

\noindent
\textbf{Solution.}
\begin{enumerate}
	\item The solution to (\ref{eq:2_norm_min}) is d = 2.
	\begin{figure}[h]
		\centering
		\includegraphics[height=7cm,width=10cm]{pro5_1.png}
		\caption{Squared singular values of A}
		\label{5}
	\end{figure}
	
	\newpage
	\item N = 10, D = 2016, we can calculate the compression
	\begin{figure}[h]
		\centering
		\includegraphics[height=7cm,width=10cm]{pro5_2.png}
		\caption{The function F}
		\label{5}
	\end{figure}\\
From the results above, we can get that $d^* = \left[1; 2; 3; 4\right]$
	\begin{table}[htb]
		\centering
		\begin{tabular}{|l|l|l|l|l|}
			\hline
			\multicolumn{1}{|c|}{$\tau$} & 0.1 & 0.05 & 0.02 & 0.005 \\ \hline
			$d^*$ & 1 & 2 & 3 & 4 \\ \hline
			Compression rate   & 10.05\%  &  20.11\%  & 30.16\%  & 40.22\% \\ \hline
		\end{tabular}
		\caption{The compression rate with respect different threshold}
	\end{table}


\end{enumerate}

\end{document}